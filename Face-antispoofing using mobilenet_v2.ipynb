{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "internship_DSPLabs_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T1GCzzgsTuC"
      },
      "source": [
        "# Face-antispoofing using mobilenet_v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOefT4Tv3rUe"
      },
      "source": [
        "## Required libraries and defined functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjoHsCld0Q_6"
      },
      "source": [
        "pip install facenet-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px1Ak9UrcA_4"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive, files\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from skimage import io, color\n",
        "\n",
        "import re\n",
        "\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "import zipfile\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "import traceback\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from operator import getitem\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from PIL import Image\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3q00aKyS2rR"
      },
      "source": [
        "def clear_dir(dir_path):\n",
        "    for root, dirs, files in os.walk(dir_path):\n",
        "        for f in files:\n",
        "            os.unlink(os.path.join(root, f))\n",
        "        for d in dirs:\n",
        "            shutil.rmtree(os.path.join(root, d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC1pHMpe2y_b"
      },
      "source": [
        "def init_random_seed(value=0):\n",
        "    random.seed(value)\n",
        "    np.random.seed(value)\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuZ0_cKmuT3v"
      },
      "source": [
        "def show_image(path, title=''):\n",
        "    plt.imshow(Image.open(path))\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnAG-mpiz1-4"
      },
      "source": [
        "def show_images(paths, subtitle, size, columns = 3):\n",
        "    rows = ceil(len(paths) / columns)\n",
        "    fig = pylab.figure(figsize=size)\n",
        "    fig.suptitle(subtitle)\n",
        "    for i in range(len(paths)):\n",
        "        fig.add_subplot(rows, columns, i + 1)\n",
        "        img = Image.open(paths[i])\n",
        "        pylab.imshow(img)\n",
        "        pylab.axis('off')\n",
        "    pylab.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7iBeMPXYMy4"
      },
      "source": [
        "def copy_data_to_device(data, device):\n",
        "    if torch.is_tensor(data):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, (list, tuple)):\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\n",
        "    raise ValueError('Invalid data type {}'.format(type(data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evob-YqK4UpD"
      },
      "source": [
        "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
        "                    lr=1e-4, epoch_n=10, batch_size_train=32, batch_size_val=32,\n",
        "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000,\n",
        "                    max_batches_per_epoch_val=1000,\n",
        "                    data_loader_ctor=DataLoader,\n",
        "                    optimizer_ctor=None,\n",
        "                    lr_scheduler_ctor=None,\n",
        "                    shuffle_train=True,\n",
        "                    dataloader_workers_n=0):\n",
        "    \"\"\"\n",
        "    Loop for model training. After each epoch, the quality of the model is assessed by validation sampling.\n",
        "    :param model: torch.nn.Module - training model\n",
        "    :param train_dataset: torch.utils.data.Dataset - train data\n",
        "    :param val_dataset: torch.utils.data.Dataset - validation data\n",
        "    :param criterion: loss function\n",
        "    :param lr: learning rate\n",
        "    :param epoch_n: maximum number of epochs\n",
        "    :param batch_size_train: number of examples processed by the model per iteration for train\n",
        "    :param batch_size_val: number of examples processed by the model per iteration for validation\n",
        "    :param device: cuda/cpu - device for calculations\n",
        "    :param early_stopping_patience: maximum number of epochs no model improvement is allowed to continue learning\n",
        "    :param l2_reg_alpha: L2-regularization coefficient\n",
        "    :param max_batches_per_epoch_train: maximum number of iterations per training epoch\n",
        "    :param max_batches_per_epoch_val: maximum number of iterations per validation epoch\n",
        "    :param data_loader_ctor: function for creating an object that converts a dataset into batches\n",
        "        (torch.utils.data.DataLoader as dafault)\n",
        "    :return: a tuple of two elements:\n",
        "         - the average value of the validation loss function at the best epoch\n",
        "         - the best model\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    if optimizer_ctor is None:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    else:\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
        "\n",
        "    if lr_scheduler_ctor is not None:\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
        "    else:\n",
        "        lr_scheduler = None\n",
        "\n",
        "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size_train, shuffle=shuffle_train,\n",
        "                                        num_workers=dataloader_workers_n)\n",
        "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size_val, shuffle=False,\n",
        "                                      num_workers=dataloader_workers_n)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "        try:\n",
        "            epoch_start = datetime.datetime.now()\n",
        "            print('Epoch {}'.format(epoch_i))\n",
        "\n",
        "            model.train()\n",
        "            mean_train_loss = 0\n",
        "            train_batches_n = 0\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_train:\n",
        "                    break\n",
        "\n",
        "                batch_x = copy_data_to_device(batch_x, device)\n",
        "                batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                pred = model(batch_x)\n",
        "                loss = criterion(pred, batch_y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                mean_train_loss += float(loss)\n",
        "                train_batches_n += 1\n",
        "\n",
        "            mean_train_loss /= train_batches_n\n",
        "            print('{} iterations, {:0.2f} sec'.format(train_batches_n,\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "            print('Average value of the train loss function:', mean_train_loss)\n",
        "\n",
        "\n",
        "            model.eval()\n",
        "            mean_val_loss = 0\n",
        "            val_batches_n = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                    if batch_i > max_batches_per_epoch_val:\n",
        "                        break\n",
        "\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                    pred = model(batch_x)\n",
        "                    loss = criterion(pred, batch_y)\n",
        "\n",
        "                    mean_val_loss += float(loss)\n",
        "                    val_batches_n += 1\n",
        "\n",
        "            mean_val_loss /= val_batches_n\n",
        "            print('Average value of the validation loss function:', mean_val_loss)\n",
        "\n",
        "            if mean_val_loss < best_val_loss:\n",
        "                best_epoch_i = epoch_i\n",
        "                best_val_loss = mean_val_loss\n",
        "                best_model = copy.deepcopy(model)\n",
        "                print('New best model!')\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "                print('The model has not improved over the last {} epochs, stop training'.format(\n",
        "                    early_stopping_patience))\n",
        "                break\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            print()\n",
        "        except KeyboardInterrupt:\n",
        "            print('Stopped early by user')\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            print('Error while training: {}\\n{}'.format(ex, traceback.format_exc()))\n",
        "            break\n",
        "\n",
        "    return best_val_loss, best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p74hcq7LYwIK"
      },
      "source": [
        "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
        "    \"\"\"\n",
        "    :param model: torch.nn.Module - trained model\n",
        "    :param dataset: torch.utils.data.Dataset - data for applying the model\n",
        "    :param device: cuda/cpu - device for calculationg\n",
        "    :param batch_size: number of examples processed by the model per iteration\n",
        "    :return: numpy.array with dimension = len(dataset)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results_by_batch = []\n",
        "\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in tqdm(dataloader, total=len(dataset)/batch_size):\n",
        "            batch_x = copy_data_to_device(batch_x, device)\n",
        "\n",
        "            if return_labels:\n",
        "                labels.append(batch_y.numpy())\n",
        "\n",
        "            batch_pred = model(batch_x)\n",
        "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
        "\n",
        "    if return_labels:\n",
        "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
        "    else:\n",
        "        return np.concatenate(results_by_batch, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYKr7zY7CyWb"
      },
      "source": [
        "___\n",
        "# Facial real/spoof classification\n",
        "### Data loading:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR6ztu8w4VT5"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFgcAwz14Vam"
      },
      "source": [
        "clear_dir('/tmp')\n",
        "\n",
        "local_zip = '/content/gdrive/My Drive/Colab Notebooks/DSP_internship/DSP_internship_data.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/DSP_internship_data')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3rWc1R5bLIO"
      },
      "source": [
        "### Using MTCNN to crop faces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7vQWFp7bP5N"
      },
      "source": [
        "mtcnn = MTCNN(select_largest=False, image_size = 224, post_process=False, device='cuda:0')\n",
        "from_dir = '/tmp/DSP_internship_data'\n",
        "dest_dir = '/tmp/DSP_internship_data_cropped'\n",
        "dirs = [\"train/real\", \"train/spoof\", \"test/unknown\"]\n",
        "\n",
        "for dir in dirs:\n",
        "    os.makedirs(os.path.join(dest_dir, dir), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOasYD-udDBN"
      },
      "source": [
        "wrong_paths = [[], [], []]\n",
        "for i, dir in enumerate(dirs):\n",
        "    for filename in tqdm(os.listdir(os.path.join(from_dir, dir))):\n",
        "        try:\n",
        "            mtcnn(Image.open(os.path.join(from_dir, dir, filename)), save_path = os.path.join(dest_dir, dir, filename))\n",
        "        except:\n",
        "            wrong_paths[i].append(os.path.join(from_dir, dir, filename))            \n",
        "            if dir == \"test/unknown\":\n",
        "                shutil.copy(os.path.join(from_dir, dir, filename), os.path.join(dest_dir, dir, filename))         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faC2K7nCKTzv"
      },
      "source": [
        " ### Convertation in LAB colors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9O2Kt-3KTEZ"
      },
      "source": [
        "#for dir in dirs:\n",
        "#    for filename in tqdm(os.listdir(os.path.join(dest_dir, dir))):\n",
        "#        img = Image.open(os.path.join(dest_dir, dir, filename))\n",
        "#        img = color.rgb2lab(img)\n",
        "#        data = np.asarray(img, dtype=\"int32\")\n",
        "#        img = Image.fromarray((data * 255).astype(np.uint8))\n",
        "#        img.save(os.path.join(dest_dir, dir, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JinWTYbub1A"
      },
      "source": [
        "### Look at bad photos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJRNkwUme8X"
      },
      "source": [
        "print(\"bad train real photos:\", len(wrong_paths[0]))\n",
        "print(\"bad train spoof photos:\", len(wrong_paths[1]))\n",
        "print(\"bad test photos:\", len(wrong_paths[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9lY3bLN2iA7"
      },
      "source": [
        "show_images(wrong_paths[0], \"bad train real photos\", size = (10, 10), columns = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha9EA56p7FSR"
      },
      "source": [
        "show_images(wrong_paths[1], \"bad train spoof photos\", size = (25, 50), columns = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LZiCxg27Fqd"
      },
      "source": [
        "show_images(wrong_paths[2], \"bad test photos\", size = (15, 15), columns = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjEi00HvGG9B"
      },
      "source": [
        "### Creating a validation dataset by several ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tc0q9a5Ht8Q"
      },
      "source": [
        "base_dir = '/tmp/DSP_internship_data_cropped'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "class_names = ['real', 'spoof']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do8cf0rXPdYV"
      },
      "source": [
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzdku4viWGty"
      },
      "source": [
        "### Look at amount of real/spoof photos for every id to find ids appropriate for validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-WQSruoWV6J"
      },
      "source": [
        "check_ids = []\n",
        "imgs_to_remove = []\n",
        "for i in range(200):\n",
        "    check_ids.append(str(i))\n",
        "\n",
        "check_dict = collections.defaultdict(lambda: collections.defaultdict(int)) # dict of dicts with int values\n",
        "\n",
        "for class_name in class_names:\n",
        "    source_dir = os.path.join(train_dir, class_name)\n",
        "    for id in check_ids:\n",
        "        paths = [f for f in os.listdir(source_dir) if re.search('_id' + id + '_', f)]\n",
        "        class_count = len(paths)\n",
        "        if class_count > 200:\n",
        "            imgs_to_remove += list(np.random.choice(np.asarray(paths), class_count - 200, replace = False))\n",
        "            class_count = 200    \n",
        "        check_dict[id][class_name] = class_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkSNPjW_qt_K"
      },
      "source": [
        "for p in imgs_to_remove:\n",
        "    os.remove(os.path.join(base_dir, \"train\", \"spoof\", p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXWqJSXYh0tE"
      },
      "source": [
        "#for s in sorted(check_dict.items(), key=lambda x: abs(getitem(x[1],'real') - getitem(x[1],'spoof'))):\n",
        "#    if (getitem(s[1], 'spoof') != 0 and getitem(s[1], 'real') != 0):\n",
        "#        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6k7QTeMbVIk"
      },
      "source": [
        "val_ids = [] # ids for validation\n",
        "val_s = 0\n",
        "i = 0\n",
        "check_dict_sorted = sorted(check_dict.items(), key=lambda x: abs(getitem(x[1],'real') - getitem(x[1],'spoof')))\n",
        "while val_s <= 1000:\n",
        "    if check_dict_sorted[i][1]['spoof'] != 0 and check_dict_sorted[i][1]['real'] != 0:\n",
        "        val_ids.append(\"_id\" + str(check_dict_sorted[i][0]) + \"_\")\n",
        "        val_s += check_dict_sorted[i][1]['spoof'] + check_dict_sorted[i][1]['real']\n",
        "    i += 1\n",
        "\n",
        "#print(val_ids) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXFOazhGEB6"
      },
      "source": [
        "regexp = re.compile(\"(\" + \"|\".join(val_ids) + \")\")\n",
        "\n",
        "for class_name in class_names:\n",
        "    source_dir = os.path.join(train_dir, class_name)\n",
        "    for file_name in tqdm(os.listdir(source_dir)):\n",
        "        if regexp.search(file_name):\n",
        "            shutil.move(os.path.join(source_dir, file_name), os.path.join(val_dir, class_name, file_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfaLkPLaJTdu"
      },
      "source": [
        "print('total training real photos:', len(os.listdir(os.path.join(train_dir, class_names[0]))))\n",
        "print('total training spoof photos:', len(os.listdir(os.path.join(train_dir, class_names[1]))))\n",
        "print('total validation real photos:', len(os.listdir(os.path.join(val_dir, class_names[0]))))\n",
        "print('total validation spoof photos:', len(os.listdir(os.path.join(val_dir, class_names[1]))))\n",
        "print('total test photos:', len(os.listdir(os.path.join(test_dir, 'unknown'))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro9OYqAqEyDJ"
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    #transforms.RandomVerticalFlip(0.25),\n",
        "    transforms.ToTensor(), # Changes dimension from H, W, C to C, H, W, where C - number of channels, H - height, W - width\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization constants\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, train_transforms)\n",
        "val_dataset = ImageFolder(val_dir, val_transforms)\n",
        "test_dataset = ImageFolder(test_dir, val_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWyXaB2pVFKg"
      },
      "source": [
        "### Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2mx-aVEPF_n"
      },
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Disable grad for all conv layers\n",
        "#for param in model.parameters():\n",
        "#    param.requires_grad = False\n",
        "\n",
        "num_features = model.classifier[1].in_features\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    torch.nn.Dropout(0.5),\n",
        "    torch.nn.Linear(num_features, 512),\n",
        "    torch.nn.BatchNorm1d(512),\n",
        "    torch.nn.LeakyReLU(),\n",
        "    torch.nn.Dropout(0.2),\n",
        "    torch.nn.Linear(512, 128),\n",
        "    torch.nn.BatchNorm1d(128),\n",
        "    torch.nn.LeakyReLU(),\n",
        "    torch.nn.Dropout(0.2),\n",
        "    torch.nn.Linear(128, 2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG5HLZyPWa9Y"
      },
      "source": [
        "### [Model training](#scrollTo=evob-YqK4UpD&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLG2Uo1GPGIC"
      },
      "source": [
        "#scheduler = lambda optim: \\\n",
        "#    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.75, verbose=True)\n",
        "\n",
        "scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.StepLR(optim, step_size=2, gamma=0.9)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=val_dataset,\n",
        "                                            criterion=functional.cross_entropy,\n",
        "                                            lr=1e-4,\n",
        "                                            epoch_n=500,\n",
        "                                            batch_size_train=203, #29\n",
        "                                            batch_size_val=92,\n",
        "                                            early_stopping_patience=20,\n",
        "                                            l2_reg_alpha=0,\n",
        "                                            lr_scheduler_ctor=scheduler,\n",
        "                                            dataloader_workers_n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QCe2ARjszd"
      },
      "source": [
        "### [Quality evaluation](#scrollTo=p74hcq7LYwIK&line=2&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q7KfnQMX4CT"
      },
      "source": [
        "train_pred, train_labels = predict_with_model(best_model, train_dataset, batch_size=203, return_labels=True)\n",
        "\n",
        "train_loss = functional.cross_entropy(torch.from_numpy(train_pred),\n",
        "                             torch.from_numpy(train_labels).long())\n",
        "\n",
        "print('Average value of the train loss:', float(train_loss))\n",
        "print('f1 train score:', f1_score(train_labels, train_pred.argmax(-1)))\n",
        "print()\n",
        "\n",
        "val_pred, val_labels = predict_with_model(best_model, val_dataset, batch_size=92, return_labels=True)\n",
        "\n",
        "val_loss = functional.cross_entropy(torch.from_numpy(val_pred),\n",
        "                            torch.from_numpy(val_labels).long())\n",
        "\n",
        "print('Average value of the validation loss:', float(val_loss))\n",
        "print('f1 val score:', f1_score(val_labels, val_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnpuOL7rncfD"
      },
      "source": [
        "### Predictions for test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVI8ma3ns5Em"
      },
      "source": [
        "test_pred = predict_with_model(best_model, test_dataset, batch_size=200, return_labels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JePg9U4tGfCN"
      },
      "source": [
        "test_predictions = torch.nn.functional.softmax(torch.from_numpy(test_pred), dim=1)[:,1].data.cpu().numpy()\n",
        "submission_df = pd.DataFrame.from_dict({'filename': list(map(os.path.basename, list(zip(*test_dataset.imgs))[0])), 'prediction': test_predictions})\n",
        "submission_df.to_csv('DSPLabs_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWhuNnOQKblH"
      },
      "source": [
        "files.download('DSPLabs_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S-xgXLjHjy6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}